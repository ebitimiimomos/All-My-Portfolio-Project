{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -q segmentation-models-pytorch\n",
    "!pip3 install -q torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 1: importig all the nessesary liabaries for the assignment\n",
    "import glob\n",
    "import cv2\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms as T\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision import models\n",
    "\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "import segmentation_models_pytorch as smp\n",
    "import torchvision\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transform\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.utils import make_grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cudu\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: uploading the project file for a saved folder\n",
    "label_colors_path = 'Cam101/label_colors.txt'\n",
    "train_images_path = 'Cam101/train'\n",
    "test_images_path = 'Cam101/test'\n",
    "\n",
    "# This code below opens the dataset from the path\n",
    "class_to_color = {}\n",
    "\n",
    "with open(label_colors_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        parts = line.strip().split('\\t')\n",
    "        r, g, b = map(int, parts[0].split(' '))\n",
    "        class_name = parts[1]\n",
    "        class_to_color[class_name] = (r, g, b)\n",
    "        \n",
    "print(\"Class to color mapping:\")\n",
    "for class_name, color in class_to_color.items():\n",
    "    print(f\"{class_name}: {color}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the code prints out the number of training and testing samples in the dataset\n",
    "\n",
    "train_images = glob.glob(os.path.join(train_images_path, '*.png'))\n",
    "train_images = [img for img in train_images if not img.endswith('_L.png')]\n",
    "train_labels = [img.replace('.png', '_L.png') for img in train_images]\n",
    "\n",
    "test_images = glob.glob(os.path.join(test_images_path, '*.png'))\n",
    "test_images = [img for img in test_images if not img.endswith('_L.png')]\n",
    "test_labels = [img.replace('.png', '_L.png') for img in test_images]\n",
    "\n",
    "print(f\"Number of training samples: {len(train_images)}\")\n",
    "print(f\"Number of testing samples: {len(test_images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the code plots a figure from the data set to visualise a sample \n",
    "\n",
    "def visualize_sample(image_path, label_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    label_img = cv2.imread(label_path)\n",
    "    label_img = cv2.cvtColor(label_img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    ax[0].imshow(img)\n",
    "    ax[0].set_title(\"Original Image\")\n",
    "    ax[1].imshow(label_img)\n",
    "    ax[1].set_title(\"Labeled Image\")\n",
    "    plt.show()\n",
    "\n",
    "# visualise a few samples\n",
    "for i in range(1):\n",
    "    visualize_sample(train_images[i], train_labels[i])\n",
    "\n",
    "#defining a data set\n",
    "train_dataset = []\n",
    "validation_dataset = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preforming data augmentation on the images and the labels \n",
    "def augment_data(images, labels, augment=True):\n",
    "    new_size = (512, 512) \n",
    "\n",
    "    for i, (x_path, y_path) in tqdm(enumerate(zip(images, labels)), total=len(images)):\n",
    "        x_name = os.path.splitext(os.path.basename(x_path))[0]\n",
    "        \n",
    "        # reading the image and the label using OpenCV\n",
    "        x = cv2.imread(x_path, cv2.IMREAD_COLOR)\n",
    "        y = cv2.imread(y_path, cv2.IMREAD_COLOR)\n",
    "\n",
    "        if augment:\n",
    "            aug = HorizontalFlip(p=1.0) \n",
    "            augmented = aug(image=x, mask=y) \n",
    "            x1 = augmented['image']\n",
    "            y1 = augmented['mask']\n",
    "\n",
    "            # apply a rotation at a random angle between 0 and 45 degrees\n",
    "            aug = Rotate(limit=45, p=1.0)\n",
    "            augmented = aug(image=x, mask=y)\n",
    "            x2 = augmented['image']\n",
    "            y2 = augmented['mask']\n",
    "\n",
    "            # append the original, horizontally flipped and rotated images and labels to lists X and Y respectively\n",
    "            X = [x, x1, x2]\n",
    "            Y = [y, y1, y2]\n",
    "        else:\n",
    "            X = [x]\n",
    "            Y = [y]\n",
    "\n",
    "        # iterate over the images and labels in X and Y\n",
    "        for index, (img, lab) in enumerate(zip(X, Y)):\n",
    "            img = cv2.resize(img, new_size)\n",
    "            lab = cv2.resize(lab, new_size)\n",
    "\n",
    "    return(augment_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#segmantation function is used to load the image and the label pairs for a given path\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_paths, label_paths, transform=None, label_transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.label_paths = label_paths\n",
    "        self.transform = transform\n",
    "        self.label_transform = label_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        label_path = self.label_paths[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        label = Image.open(label_path).convert('L')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.label_transform:\n",
    "            label = self.label_transform(label)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# applying transformation to the images\n",
    "mytransformsImage = transform.Compose([\n",
    "    transform.Resize((736, 960)),\n",
    "    transform.ToTensor(),\n",
    "    transform.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "mytransformsLabel = transform.Compose([\n",
    "    transform.Resize((736, 960)),\n",
    "    transform.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the tranning data into traning and validation by 90% being traning and 10% being validation\n",
    "train_val_images = glob.glob(os.path.join(train_images_path, '*.png'))\n",
    "train_val_images = [img for img in train_val_images if not img.endswith('_L.png')]\n",
    "train_val_labels = [img.replace('.png', '_L.png') for img in train_val_images]\n",
    "train_val = list(zip(train_val_images, train_val_labels))\n",
    "random.shuffle(train_val)\n",
    "train_size = int(0.9 * len(train_val))\n",
    "train_images, train_labels = zip(*train_val[:train_size])\n",
    "val_images, val_labels = zip(*train_val[train_size:])\n",
    "\n",
    "#preparing the data for segmentation tasks \n",
    "train_dataset = SegmentationDataset(train_images, train_labels, transform=mytransformsImage, label_transform=mytransformsLabel)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0)\n",
    "\n",
    "val_dataset = SegmentationDataset(val_images, val_labels, transform=mytransformsImage, label_transform=mytransformsLabel)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "test_dataset = SegmentationDataset(test_images, test_labels, transform=mytransformsImage, label_transform=mytransformsLabel)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(val_dataset)}\")\n",
    "print(f\"Number of testing samples: {len(test_dataset)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BUILDING THE MODELS- IMPORTING THE MODELS UNet, DeepLabs and FCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates the pixel accuracy of the segmintation\n",
    "def pixel_accuracy(output, mask):\n",
    "    with torch.no_grad():\n",
    "        _, predicted_labels = torch.max(output, dim=1)\n",
    "        correct = torch.eq(predicted_labels, mask)\n",
    "        accuracy = torch.mean(correct.float())\n",
    "    return accuracy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used to calculate the mIou of the segmintation\n",
    "def mIoU(pred_mask, mask, smooth=1e-10, n_classes=23):\n",
    "    with torch.no_grad():\n",
    "        pred_mask = F.softmax(pred_mask, dim=1)\n",
    "        pred_mask = torch.argmax(pred_mask, dim=1)\n",
    "        pred_mask = pred_mask.view(-1)\n",
    "        mask = mask.view(-1)\n",
    "\n",
    "        iou_per_class = []\n",
    "        for clas in range(n_classes):  # loop per pixel class\n",
    "            true_class = torch.eq(pred_mask, clas)\n",
    "            true_label = torch.eq(mask, clas)\n",
    "\n",
    "            if torch.sum(true_label).item() == 0:  # no existing label in this class\n",
    "                iou_per_class.append(np.nan)\n",
    "            else:\n",
    "                intersect = torch.logical_and(true_class, true_label).sum().float().item()\n",
    "                union = torch.logical_or(true_class, true_label).sum().float().item()\n",
    "\n",
    "                iou = (intersect + smooth) / (union + smooth)\n",
    "                iou_per_class.append(iou)\n",
    "        return np.nanmean(iou_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def fit(epochs, model, train_loader, val_loader, criterion, optimizer, scheduler, patch=False):\n",
    "    torch.cuda.empty_cache()\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    val_iou = []\n",
    "    val_acc = []\n",
    "    train_iou = []\n",
    "    train_acc = []\n",
    "    lrs = []\n",
    "    min_loss = np.inf\n",
    "    decrease = 1\n",
    "    not_improve = 0\n",
    "\n",
    "    model.to(device)\n",
    "    fit_time = time.time()\n",
    "    for e in range(epochs):\n",
    "        since = time.time()\n",
    "        running_loss = 0\n",
    "        iou_score = 0\n",
    "        accuracy = 0\n",
    "        \n",
    "        # training loop\n",
    "        model.train()\n",
    "        for i, data in enumerate(tqdm(train_loader)):\n",
    "            # training phase\n",
    "            image_tiles, mask_tiles = data\n",
    "            if patch:\n",
    "                bs, n_tiles, c, h, w = image_tiles.size()\n",
    "                image_tiles = image_tiles.view(-1, c, h, w)\n",
    "                mask_tiles = mask_tiles.view(-1, h, w)\n",
    "\n",
    "            image = image_tiles.to(device)\n",
    "            mask = mask_tiles.to(device)\n",
    "            # forward\n",
    "            output = model(image)\n",
    "            loss = criterion(output, mask.squeeze().long())\n",
    "            # evaluation metrics\n",
    "            iou_score += mIoU(output, mask.squeeze()) \n",
    "            accuracy += pixel_accuracy(output, mask.squeeze()) \n",
    "            # backward\n",
    "            loss.backward()\n",
    "            optimizer.step()  # update weight\n",
    "            optimizer.zero_grad()  # reset gradient\n",
    "\n",
    "            # step the learning rate\n",
    "            lrs.append(get_lr(optimizer))\n",
    "            scheduler.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        else:\n",
    "            model.eval()\n",
    "            test_loss = 0\n",
    "            test_accuracy = 0\n",
    "            val_iou_score = 0\n",
    "            # validation loop\n",
    "            with torch.no_grad():\n",
    "                for i, data in enumerate(tqdm(val_loader)):\n",
    "                    # reshape to 9 patches from single image, delete batch size\n",
    "                    image_tiles, mask_tiles = data\n",
    "\n",
    "                    if patch:\n",
    "                        bs, n_tiles, c, h, w = image_tiles.size()\n",
    "                        image_tiles = image_tiles.view(-1, c, h, w)\n",
    "                        mask_tiles = mask_tiles.view(-1, h, w)\n",
    "\n",
    "                    image = image_tiles.to(device)\n",
    "                    mask = mask_tiles.to(device)\n",
    "                    output = model(image)\n",
    "                    # evaluation metrics\n",
    "                    val_iou_score += mIoU(output, mask.squeeze()) \n",
    "                    test_accuracy += pixel_accuracy(output, mask.squeeze()) \n",
    "                    # loss\n",
    "                    loss = criterion(output, mask.squeeze())  \n",
    "                    test_loss += loss.item()\n",
    "\n",
    "            # calculation mean for each batch\n",
    "            train_losses.append(running_loss / len(train_loader))\n",
    "            test_losses.append(test_loss / len(val_loader))\n",
    "            \n",
    "            if min_loss > (test_loss / len(val_loader)):\n",
    "                print('Loss Decreasing.. {:.3f} >> {:.3f} '.format(min_loss, (test_loss / len(val_loader))))\n",
    "            min_loss = (test_loss / len(val_loader))\n",
    "            decrease += 1\n",
    "            if decrease % 5 == 0:\n",
    "                print('saving model...')\n",
    "                torch.save(model, 'Unet-Mobilenet_v2_mIoU-{:.3f}.pt'.format(val_iou_score / len(val_loader)))\n",
    "\n",
    "        if (test_loss / len(val_loader)) > min_loss:\n",
    "            not_improve += 1\n",
    "            min_loss = (test_loss / len(val_loader))\n",
    "            print(f'Loss Not Decrease for {not_improve} time')\n",
    "            if not_improve == 7:\n",
    "                print('Loss not decrease for 7 times, Stop Training')\n",
    "                break\n",
    "\n",
    "        # iou\n",
    "        val_iou.append(val_iou_score / len(val_loader))\n",
    "        train_iou.append(iou_score / len(train_loader))\n",
    "        train_acc.append(accuracy / len(train_loader))\n",
    "        val_acc.append(test_accuracy / len(val_loader))\n",
    "        print(\"Epoch:{}/{}..\".format(e + 1, epochs),\n",
    "              \"Train Loss: {:.3f}..\".format(running_loss / len(train_loader)),\n",
    "              \"Val Loss: {:.3f}..\".format(test_loss / len(val_loader)),\n",
    "              \"Train mIoU:{:.3f}..\".format(iou_score / len(train_loader)),\n",
    "              \"Val mIoU: {:.3f}..\".format(val_iou_score / len(val_loader)),\n",
    "              \"Train Acc:{:.3f}..\".format(accuracy / len(train_loader)),\n",
    "              \"Val Acc:{:.3f}..\".format(test_accuracy / len(val_loader)),\n",
    "              \"Time: {:.2f}m\".format((time.time() - since) / 60))\n",
    "\n",
    "    history = {'train_loss': train_losses, 'val_loss': test_losses,\n",
    "           'train_miou': train_iou, 'val_miou': val_iou,\n",
    "           'train_acc': train_acc, 'val_acc': val_acc,\n",
    "           'lrs': lrs}\n",
    "    print('Total time: {:.2f} m'.format((time.time() - fit_time) / 60))\n",
    "    return history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_pred, y_true):\n",
    "  \n",
    "  y_pred = y_pred.cpu().detach().numpy()\n",
    "  y_pred = y_pred > 0.5\n",
    "  y_pred = y_pred.astype(np.uint8)\n",
    "  y_pred = y_pred.reshape(-1)\n",
    "  \n",
    "  y_true = y_true.cpu().detach().numpy()\n",
    "  y_true = y_true > 0.5\n",
    "  y_true = y_true.astype(np.uint8)\n",
    "  y_true = y_true.reshape(-1)\n",
    "\n",
    "  # Compute the Jaccard/Intersection over Union (IoU) and pixel accuracy\n",
    "  IoU = jaccard_score(y_true, y_pred) \n",
    "  accuracy = accuracy_score(y_true, y_pred) \n",
    "\n",
    "  return IoU, accuracy\n",
    "\n",
    "def run_time(start_time, end_time):\n",
    "\n",
    "  time_taken = end_time - start_time\n",
    "  mins = int(time_taken/60)\n",
    "  secs = int(time_taken - (mins * 60))\n",
    "  \n",
    "  return mins, secs\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Testing the models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visulaing the results \n",
    "def show(img,output,label,denorm = False):\n",
    "    img,output,label = img.cpu(),output.cpu(),label.cpu()\n",
    "    fig,ax = plt.subplots(len(output),3,figsize=(15,30))\n",
    "    cols = ['Input Image','Actual Output','Predicted Output']\n",
    "    for i in range(len(output)):\n",
    "        if(len(output) == 3):\n",
    "            Img,Lab,act = img[i],output[i],label[i]\n",
    "            Img,Lab,act = Img,Lab.detach().permute(1,2,0).numpy(),act\n",
    "            ax[i][0].imshow(Img.permute(1,2,0))\n",
    "            ax[i][2].imshow(Lab)\n",
    "            ax[i][1].imshow(act.permute(1,2,0))\n",
    "        else:\n",
    "            Img,Lab,act = img[i],output[i],label[i]\n",
    "            Img,Lab,act = Img,Lab.detach().permute(1,2,0).numpy(),act\n",
    "            ax[0].imshow(Img.permute(1,2,0))\n",
    "            ax[2].imshow(Lab)\n",
    "            ax[1].imshow(act.permute(1,2,0))\n",
    "            #ax[0].title('this')\n",
    "            for ax, col in zip(ax, cols):\n",
    "                ax.set_title(col)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UNets model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convblock(nn.Module):\n",
    "    \n",
    "      def __init__(self,input_channel,output_channel,kernal=3,stride=1,padding=1):\n",
    "            \n",
    "        super().__init__()\n",
    "        self.convblock = nn.Sequential(\n",
    "            nn.Conv2d(input_channel,output_channel,kernal,stride,padding),\n",
    "            nn.BatchNorm2d(output_channel),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(output_channel,output_channel,kernal),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    \n",
    "\n",
    "      def forward(self,x):\n",
    "        x = self.convblock(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_channel,retain=True):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = Convblock(input_channel,32)\n",
    "        self.conv2 = Convblock(32,64)\n",
    "        self.conv3 = Convblock(64,128)\n",
    "        self.conv4 = Convblock(128,256)\n",
    "        self.neck = nn.Conv2d(256,512,3,1)\n",
    "        self.upconv4 = nn.ConvTranspose2d(512,256,3,2,0,1)\n",
    "        self.dconv4 = Convblock(512,256)\n",
    "        self.upconv3 = nn.ConvTranspose2d(256,128,3,2,0,1)\n",
    "        self.dconv3 = Convblock(256,128)\n",
    "        self.upconv2 = nn.ConvTranspose2d(128,64,3,2,0,1)\n",
    "        self.dconv2 = Convblock(128,64)\n",
    "        self.upconv1 = nn.ConvTranspose2d(64,32,3,2,0,1)\n",
    "        self.dconv1 = Convblock(64,32)\n",
    "        self.out = nn.Conv2d(32,3,1,1)\n",
    "        self.retain = retain\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        # Encoder Network\n",
    "        \n",
    "        # Conv down 1\n",
    "        conv1 = self.conv1(x)\n",
    "        pool1 = F.max_pool2d(conv1,kernel_size=2,stride=2)\n",
    "        # Conv down 2\n",
    "        conv2 = self.conv2(pool1)\n",
    "        pool2 = F.max_pool2d(conv2,kernel_size=2,stride=2)\n",
    "        # Conv down 3\n",
    "        conv3 = self.conv3(pool2)\n",
    "        pool3 = F.max_pool2d(conv3,kernel_size=2,stride=2)\n",
    "        # Conv down 4\n",
    "        conv4 = self.conv4(pool3)\n",
    "        pool4 = F.max_pool2d(conv4,kernel_size=2,stride=2)\n",
    "\n",
    "        # BottelNeck\n",
    "        neck = self.neck(pool4)\n",
    "        \n",
    "        # Decoder Network\n",
    "        \n",
    "        # Upconv 1\n",
    "        upconv4 = self.upconv4(neck)\n",
    "        croped = self.crop(conv4,upconv4)\n",
    "        # Making the skip connection 1\n",
    "        dconv4 = self.dconv4(torch.cat([upconv4,croped],1))\n",
    "        # Upconv 2\n",
    "        upconv3 = self.upconv3(dconv4)\n",
    "        croped = self.crop(conv3,upconv3)\n",
    "        # Making the skip connection 2\n",
    "        dconv3 = self.dconv3(torch.cat([upconv3,croped],1))\n",
    "        # Upconv 3\n",
    "        upconv2 = self.upconv2(dconv3)\n",
    "        croped = self.crop(conv2,upconv2)\n",
    "        # Making the skip connection 3\n",
    "        dconv2 = self.dconv2(torch.cat([upconv2,croped],1))\n",
    "        # Upconv 4\n",
    "        upconv1 = self.upconv1(dconv2)\n",
    "        croped = self.crop(conv1,upconv1)\n",
    "        # Making the skip connection 4\n",
    "        dconv1 = self.dconv1(torch.cat([upconv1,croped],1))\n",
    "        # Output Layer\n",
    "        out = self.out(dconv1)\n",
    "        \n",
    "        if self.retain == True:\n",
    "            out = F.interpolate(out,list(x.shape)[2:])\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "    def crop(self,input_tensor,target_tensor):\n",
    "        # For making the size of the encoder conv layer and the decoder Conv layer same\n",
    "        _,_,H,W = target_tensor.shape\n",
    "        return transform.CenterCrop([H,W])(input_tensor)\n",
    "    \n",
    "# initializing the model\n",
    "model = UNet(3).float().to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialising the UNets model\n",
    "uent = UNet(3).float().to(device) \n",
    "\n",
    "lr = 0.01\n",
    "epochs = 20\n",
    "\n",
    "# Choosing the loss function to be Mean Square Error Loss\n",
    "lossfunc = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "# Set other parameters and create optimiser and loss function\n",
    "\n",
    "for i in range(epochs):\n",
    "    trainloss = 0\n",
    "    valloss = 0\n",
    "\n",
    "#traning the model\n",
    "    for img, label in tqdm(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "        output = model(img)\n",
    "        loss = lossfunc(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        trainloss += loss.item()\n",
    "\n",
    "    if i % 5 == 0:\n",
    "        show(img, output, label)\n",
    "\n",
    "    train_loss.append(trainloss / len(train_dataloader))\n",
    "\n",
    "   # disabling the  multiprocessing for validation data loader to provent it from running at a high RAM\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=4, num_workers=0)\n",
    "\n",
    "#validating the model\n",
    "    for img, label in tqdm(val_dataloader):\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "        output = model(img)\n",
    "        loss = lossfunc(output, label)\n",
    "        valloss += loss.item()\n",
    "\n",
    "    val_loss.append(valloss / len(val_dataloader))\n",
    "\n",
    "    print(\"epoch: {}, train loss: {}, valid loss: {}\".format(i, train_loss[-1], val_loss[-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the training vs validation loss curve for UNets\n",
    "plt.plot(train_loss,color='b',label='train loss')\n",
    "plt.plot(val_loss,color='r',label = 'val_loss')\n",
    "plt.legend()\n",
    "\n",
    "#visulaing the results \n",
    "def show(img,output,label,denorm = False):\n",
    "    img,output,label = img.cpu(),output.cpu(),label.cpu()\n",
    "    fig,ax = plt.subplots(len(output),3,figsize=(15,30))\n",
    "    cols = ['Input Image','Actual Output','Predicted Output']\n",
    "    for i in range(len(output)):\n",
    "        if(len(output) == 3):\n",
    "            Img,Lab,act = img[i],output[i],label[i]\n",
    "            Img,Lab,act = Img,Lab.detach().permute(1,2,0).numpy(),act\n",
    "            ax[i][0].imshow(Img.permute(1,2,0))\n",
    "            ax[i][2].imshow(Lab)\n",
    "            ax[i][1].imshow(act.permute(1,2,0))\n",
    "        else:\n",
    "            Img,Lab,act = img[i],output[i],label[i]\n",
    "            Img,Lab,act = Img,Lab.detach().permute(1,2,0).numpy(),act\n",
    "            ax[0].imshow(Img.permute(1,2,0))\n",
    "            ax[2].imshow(Lab)\n",
    "            ax[1].imshow(act.permute(1,2,0))\n",
    "            #ax[0].title('this')\n",
    "            for ax, col in zip(ax, cols):\n",
    "                ax.set_title(col)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a graph with itertion vs epoch for UNets\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(val_loss, label='Validation Loss')\n",
    "\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Iteration')\n",
    "ax.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeepLabs Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, in_channels // 4, kernel_size=1)\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels // 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.ConvTranspose2d(in_channels // 4, in_channels // 4, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(in_channels // 4)\n",
    "        self.conv3 = nn.Conv2d(in_channels // 4, out_channels, kernel_size=1)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x, skip=None):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        \n",
    "        if skip is not None:\n",
    "            # Resize the skip tensor to match the size of the current tensor\n",
    "            skip = F.interpolate(skip, size=x.size()[2:], mode='bilinear', align_corners=True)\n",
    "            # Concatenate the skip tensor with the current tensor\n",
    "            x = torch.cat([x, skip], dim=1)\n",
    "        \n",
    "        x = self.relu(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "class DeepLabHead(nn.Sequential):\n",
    "\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(DeepLabHead, self).__init__(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_ch, out_ch, 1)\n",
    "        )\n",
    "\n",
    "class DeepLab(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.base_model = models.resnet50(pretrained=True)\n",
    "        self.base_model = nn.Sequential(*list(self.base_model.children())[:-2])\n",
    "        self.classifier = DeepLabHead(2048, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.base_model(x)\n",
    "        h = self.classifier(h)\n",
    "        h = F.interpolate(h, size=x.shape[2:], mode='bilinear', align_corners=False)\n",
    "        return h\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "deeplab = DeepLab(3).to(device)\n",
    "print(deeplab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialising the DeepLabs model\n",
    "model = deeplab.to(device)\n",
    "\n",
    "lr = 0.01\n",
    "epochs = 20\n",
    "\n",
    "# choosing the loss function to be Mean Square Error Loss\n",
    "lossfunc = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "# setting other parameters and create optimiser and loss function\n",
    "for i in range(epochs):\n",
    "    trainloss = 0\n",
    "    valloss = 0\n",
    "\n",
    "    for img, label in tqdm(train_dataloader):\n",
    "#traning the model\n",
    "        optimizer.zero_grad()\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "        output = model(img)\n",
    "        loss = lossfunc(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        trainloss += loss.item()\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        valloss = 0\n",
    "        for img, label in val_dataloader:\n",
    "            img = img.to(device)\n",
    "            img = img.unsqueeze(0)\n",
    "            label = label.to(device)\n",
    "            img = torch.squeeze(img, dim=0) \n",
    "            label = torch.squeeze(label, dim=0) \n",
    "            output = model(img)\n",
    "            loss = lossfunc(output, label)\n",
    "            valloss += loss.item()    \n",
    "\n",
    "\n",
    "    if i % 1 == 0:\n",
    "        show(img, output, label)\n",
    "\n",
    "    train_loss.append(trainloss / len(train_dataloader))\n",
    "\n",
    "    # disabling the  multiprocessing for validation data loader to provent it from running at a high RAM\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=4, num_workers=0)\n",
    "\n",
    "    for img, label in tqdm(val_dataloader):\n",
    "#validating the model\n",
    "        img = img.to(device)\n",
    "        \n",
    "        label = label.to(device)\n",
    "        output = model(img)\n",
    "        loss = lossfunc(output, label)\n",
    "        valloss += loss.item()\n",
    "\n",
    "    val_loss.append(valloss / len(val_dataloader))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        valloss = 0\n",
    "        for img, label in val_dataloader:\n",
    "            img = img.to(device)\n",
    "            label = label.to(device)\n",
    "            img = torch.squeeze(img, dim=0) \n",
    "            label = torch.squeeze(label, dim=0) \n",
    "            output = model(img)\n",
    "            loss = lossfunc(output, label)\n",
    "            valloss += loss.item()\n",
    "\n",
    "    val_loss.append(valloss / len(val_dataloader))\n",
    "\n",
    "\n",
    "    print(\"epoch: {}, train loss: {}, valid loss: {}\".format(i, train_loss[-1], val_loss[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the training vs validation loss curve for DeepLabs\n",
    "plt.plot(train_loss,color='b',label='train loss')\n",
    "plt.plot(val_loss,color='r',label = 'val_loss')\n",
    "plt.legend()\n",
    "\n",
    "#visulaing the results \n",
    "def show(img,output,label,denorm = False):\n",
    "    img,output,label = img.cpu(),output.cpu(),label.cpu()\n",
    "    fig,ax = plt.subplots(len(output),3,figsize=(15,30))\n",
    "    cols = ['Input Image','Actual Output','Predicted Output']\n",
    "    for i in range(len(output)):\n",
    "        if(len(output) == 3):\n",
    "            Img,Lab,act = img[i],output[i],label[i]\n",
    "            Img,Lab,act = Img,Lab.detach().permute(1,2,0).numpy(),act\n",
    "            ax[i][0].imshow(Img.permute(1,2,0))\n",
    "            ax[i][2].imshow(Lab)\n",
    "            ax[i][1].imshow(act.permute(1,2,0))\n",
    "        else:\n",
    "            Img,Lab,act = img[i],output[i],label[i]\n",
    "            Img,Lab,act = Img,Lab.detach().permute(1,2,0).numpy(),act\n",
    "            ax[0].imshow(Img.permute(1,2,0))\n",
    "            ax[2].imshow(Lab)\n",
    "            ax[1].imshow(act.permute(1,2,0))\n",
    "            #ax[0].title('this')\n",
    "            for ax, col in zip(ax, cols):\n",
    "                ax.set_title(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a graph with itertion vs epoch for DeepLabs\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(val_loss, label='Validation Loss')\n",
    "\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Iteration')\n",
    "ax.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fully convelutional Network (FCN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super().__init__()\n",
    "\n",
    "        # defining the base feature extraction layers\n",
    "        self.base = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        # defining the classifier layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Conv2d(512, 1024, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(),\n",
    "            nn.Conv2d(1024, 1024, kernel_size=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(),\n",
    "            nn.Conv2d(1024, num_classes, kernel_size=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_size = x.size()\n",
    "        x = self.base(x)\n",
    "        x = self.classifier(x)\n",
    "        return F.interpolate(x, x_size[2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "\n",
    "# Instantiate and print the FCN architecture\n",
    "fcn = FCN().to(device)\n",
    "print(fcn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialising the model\n",
    "model1 = fcn.to(device)\n",
    "\n",
    "lr = 0.01\n",
    "epochs = 20\n",
    "\n",
    "# Choosing the loss function to be Mean Square Error Loss\n",
    "lossfunc = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "# setting parameters and create optimiser and loss function\n",
    "for i in range(epochs):\n",
    "    trainloss = 0\n",
    "    valloss = 0\n",
    "\n",
    "    for img, label in tqdm(train_dataloader):\n",
    "#training model\n",
    "        optimizer.zero_grad()\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "        output = model1(img)\n",
    "        loss = lossfunc(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        trainloss += loss.item()\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        valloss = 0\n",
    "        for img, label in val_dataloader:\n",
    "            img = img.to(device)\n",
    "            img = img.unsqueeze(0)\n",
    "            label = label.to(device)\n",
    "            img = torch.squeeze(img, dim=0)\n",
    "            label = torch.squeeze(label, dim=0) \n",
    "            output = model1(img)\n",
    "            loss = lossfunc(output, label)\n",
    "            valloss += loss.item()    \n",
    "\n",
    "\n",
    "    if i % 1 == 0:\n",
    "        show(img, output, label)\n",
    "\n",
    "    train_loss.append(trainloss / len(train_dataloader))\n",
    "\n",
    "    # Disabling the multiprocessing for validation data loader to porvent it from running at a high RAM\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=4, num_workers=0)\n",
    "\n",
    "    for img, label in tqdm(val_dataloader):\n",
    "#validation model\n",
    "        img = img.to(device)\n",
    "        \n",
    "        label = label.to(device)\n",
    "        output = model(img)\n",
    "        loss = lossfunc(output, label)\n",
    "        valloss += loss.item()\n",
    "\n",
    "    val_loss.append(valloss / len(val_dataloader))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        valloss = 0\n",
    "        for img, label in val_dataloader:\n",
    "            img = img.to(device)\n",
    "            label = label.to(device)\n",
    "            img = torch.squeeze(img, dim=0) \n",
    "            label = torch.squeeze(label, dim=0) \n",
    "            output = model(img)\n",
    "            loss = lossfunc(output, label)\n",
    "            valloss += loss.item()\n",
    "\n",
    "    val_loss.append(valloss / len(val_dataloader))\n",
    "\n",
    "\n",
    "    print(\"epoch: {}, train loss: {}, valid loss: {}\".format(i, train_loss[-1], val_loss[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the training vs validation loss curve for FCN\n",
    "plt.plot(train_loss,color='b',label='train loss')\n",
    "plt.plot(val_loss,color='r',label = 'val_loss')\n",
    "plt.legend()\n",
    "\n",
    "#visulaing the results \n",
    "def show(img,output,label,denorm = False):\n",
    "    img,output,label = img.cpu(),output.cpu(),label.cpu()\n",
    "    fig,ax = plt.subplots(len(output),3,figsize=(15,30))\n",
    "    cols = ['Input Image','Actual Output','Predicted Output']\n",
    "    for i in range(len(output)):\n",
    "        if(len(output) == 3):\n",
    "            Img,Lab,act = img[i],output[i],label[i]\n",
    "            Img,Lab,act = Img,Lab.detach().permute(1,2,0).numpy(),act\n",
    "            ax[i][0].imshow(Img.permute(1,2,0))\n",
    "            ax[i][2].imshow(Lab)\n",
    "            ax[i][1].imshow(act.permute(1,2,0))\n",
    "        else:\n",
    "            Img,Lab,act = img[i],output[i],label[i]\n",
    "            Img,Lab,act = Img,Lab.detach().permute(1,2,0).numpy(),act\n",
    "            ax[0].imshow(Img.permute(1,2,0))\n",
    "            ax[2].imshow(Lab)\n",
    "            ax[1].imshow(act.permute(1,2,0))\n",
    "            #ax[0].title('this')\n",
    "            for ax, col in zip(ax, cols):\n",
    "                ax.set_title(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a graph with itertion vs epoch for FCN\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(val_loss, label='Validation Loss')\n",
    "\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Iteration')\n",
    "ax.legend(loc='best')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
