{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ek80msl5ux3g"
      },
      "source": [
        "STEP 1: Highlight the imported packages and libaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNLvDj2M2UnY"
      },
      "outputs": [],
      "source": [
        "# importing all the necessary libaries and packages\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import math\n",
        "import itertools\n",
        "import time\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.utils.data as data\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSti8KQhWIyo"
      },
      "source": [
        "STEP 2 and 3: Loading the data set using Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpR1fmjE2by2",
        "outputId": "668b2b69-d6b6-454b-d126-e59c134f69a8"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "# This loads the MNIST training data using the PyTorch datasets module, setting the root parameter to ROOT, and train parameter to True to indicate that the data is for training. \n",
        "If the data has not already been downloaded, it will be downloaded using the download parameter set to True.\n",
        "it then calculates the statistical representation of the data, which is the mean and standard deviation of the pixel values in the training data. \n",
        "This is done by first converting the data to float type and then dividing it by 255 to scale the pixel values between 0 and 1. \n",
        "The mean and standard deviation are then calculated using the PyTorch data module. \n",
        "\n",
        "'''\n",
        "\n",
        "train_data = torchvision.datasets.EMNIST(root = './data',  split='balanced', download = True,\n",
        "                                            train = True, transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.4,), (0.4,))]))\n",
        "test_data = torchvision.datasets.EMNIST(root = './data', split='balanced', download=True,\n",
        "                                            train=False, transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.4,), (0.4,))]))\n",
        "\n",
        "print(\"number of training dataset:\", len(train_data))\n",
        "print(\"number of testing dataset:\", len(test_data ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUuoF3w93KyK",
        "outputId": "42bab44a-cf95-4962-9dbf-92c83057841a"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "used to determine the proportion of the data that will be used for validation. In this case, it is set to 0.8, \n",
        "which means that 80% of the data will be used for training and 20% will be used for validation.\n",
        "'''\n",
        "\n",
        "#splitting data into traning and validation\n",
        "VALID_RATIO = 0.8 \n",
        "n_train_examples = int(len(train_data) * VALID_RATIO)\n",
        "n_valid_examples = len(train_data) - n_train_examples\n",
        "\n",
        "'''\n",
        "used to randomly split the training data into two subsets, train_data and valid_data, using the number of \n",
        "examples for each set that was calculated earlier.\n",
        "'''\n",
        "train_data, valid_data = data.random_split(train_data,\n",
        "                                           [n_train_examples, n_valid_examples])\n",
        "\n",
        "#printing out the number of examples in each set.\n",
        "print(f'Number of training examples: {len(train_data)}')\n",
        "print(f'Number of validation examples: {len(valid_data)}')\n",
        "print(f'Number of testing examples: {len(test_data)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNHnR65l43X2"
      },
      "outputs": [],
      "source": [
        "#creating  data loaders for the training and testing sets\n",
        "#defining the batche size of 250 samples, with the data being suffeled  \n",
        "batch_size = 250\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yt-Yli4_Wegi"
      },
      "source": [
        "STEP 4:  Understanding the dataset and visualising the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "3y6wjcgL7rFz",
        "outputId": "0629aaea-41db-453f-e5e5-e6ebcb684063"
      },
      "outputs": [],
      "source": [
        "# visualising the EMNIST dataset images\n",
        "classes = train_data.dataset.classes\n",
        "num_classes = len(classes)\n",
        "\n",
        "# a function to convert label index to class name\n",
        "def convert_label_to_classname(label):\n",
        "    return classes[label]\n",
        "\n",
        "# getting a batch of training data\n",
        "images, labels = next(iter(train_loader))\n",
        "\n",
        "# plotting the images from the data set (we are plotting 4)\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "for i in range(min(batch_size, 4)):\n",
        "    ax = fig.add_subplot(4, 4, i+1)\n",
        "    ax.imshow(images[i].squeeze().numpy(), cmap='gray')\n",
        "    ax.set_title(convert_label_to_classname(labels[i].item()))\n",
        "    ax.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bojGRTrmvDMW"
      },
      "source": [
        "STEP 5: Hyperparameters "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxxqFBk6-nSY"
      },
      "outputs": [],
      "source": [
        "#if a GPU systems is avalible it will be used (used for faster computation) if not then it will use CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DeqinG0c-JUY"
      },
      "outputs": [],
      "source": [
        "# defining the models parameters\n",
        "# The image size set up is 784 because that how many pixels there are in total (28*28=784)\n",
        "input_size = 784\n",
        "# The Number of nodes at hidden layer\n",
        "hidden_size = 100\n",
        "# number of output classes discrete range [0,9]\n",
        "num_classes = 47\n",
        "batch_size = 64\n",
        "lr = 0.01\n",
        "epochs = 20\n",
        "new_epoch = ...\n",
        "activation = 'relu'\n",
        "optimiser = optim.SGD\n",
        "batch_norm = True\n",
        "regularization = 0.001\n",
        "dropout = 0.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYUxZbmRjsMU"
      },
      "source": [
        "Multi Layer Perceptron (MLP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7COfoWz8cnA"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes, activation, batch_norm, dropout, regularization):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc4 = nn.Linear(hidden_size, num_classes)\n",
        "        self.activation = activation\n",
        "        self.batch_norm = batch_norm\n",
        "        self.dropout = dropout\n",
        "        self.regularization = regularization\n",
        "        if self.batch_norm:\n",
        "            self.bn1 = nn.BatchNorm1d(hidden_size)\n",
        "            self.bn2 = nn.BatchNorm1d(hidden_size)\n",
        "            self.bn3 = nn.BatchNorm1d(hidden_size)\n",
        "        if self.regularization:\n",
        "            self.l1 = nn.L1Loss()\n",
        "            self.l2 = nn.MSELoss()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        out = self.fc1(x)\n",
        "        if self.batch_norm:\n",
        "            out = self.bn1(out)\n",
        "        if self.activation == 'relu':\n",
        "            out = F.relu(out)\n",
        "        elif self.activation == 'sigmoid':\n",
        "            out = F.sigmoid(out)\n",
        "        elif self.activation == 'tanh':\n",
        "            out = F.tanh(out)\n",
        "        if self.dropout:\n",
        "            out = F.dropout(out, p=self.dropout)\n",
        "        out = self.fc2(out)\n",
        "        if self.batch_norm:\n",
        "            out = self.bn2(out)\n",
        "        if self.activation == 'relu':\n",
        "            out = F.relu(out)\n",
        "        elif self.activation == 'sigmoid':\n",
        "            out = F.sigmoid(out)\n",
        "        elif self.activation == 'tanh':\n",
        "            out = F.tanh(out)\n",
        "        if self.dropout:\n",
        "            out = F.dropout(out, p=self.dropout)\n",
        "        out = self.fc3(out)\n",
        "        if self.batch_norm:\n",
        "            out = self.bn3(out)\n",
        "        if self.activation == 'relu':\n",
        "            out = F.relu(out)\n",
        "        elif self.activation == 'sigmoid':\n",
        "            out = F.sigmoid(out)\n",
        "        elif self.activation == 'tanh':\n",
        "            out = F.tanh(out)\n",
        "        if self.dropout:\n",
        "            out = F.dropout(out, p=self.dropout)\n",
        "        out = self.fc4(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZECArRO9gMO",
        "outputId": "a6b8ab18-4f39-4de7-c624-093194805bbc"
      },
      "outputs": [],
      "source": [
        "# defining the MLP model\n",
        "model = MLP(input_size, hidden_size, num_classes, activation, batch_norm, dropout, regularization).to(device)\n",
        "\n",
        "# defining the loss function, optimiser and learning rate (lr) scheduler\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimiser = optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimiser, mode='min', factor=0.1, patience=1, verbose=True)\n",
        "\n",
        "accuracy, losses, epochs = [], [], epochs\n",
        "\n",
        "# training the model\n",
        "start_time = time.time()\n",
        "for epoch in range(epochs):\n",
        "    correct, total = 0, 0\n",
        "    for images, labels in train_loader:\n",
        "        # Move the images and labels to the device\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # regularisation\n",
        "        if regularization in ('l1', 'l2'):\n",
        "            reg_loss = sum(torch.norm(param, p=int(regularization[1])) for param in model.parameters())\n",
        "            loss += regularization * reg_loss\n",
        "        # Backward and optimize\n",
        "        optimiser.zero_grad()\n",
        "        loss.backward()\n",
        "        optimiser.step()\n",
        "        # Compute training accuracy\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    scheduler.step(loss)\n",
        "\n",
        "    # computing the training accuracy for epoch\n",
        "    accuracy.append(correct / total)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    # printing the loss and accuracy after each epoch\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}, Training Accuracy: {accuracy[-1] * 100:.2f}%\")\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "print(f\"Training time: {training_time:.2f}s\")\n",
        "\n",
        "result = list(zip(range(epochs), accuracy, losses))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfJg7zLmvPGQ"
      },
      "outputs": [],
      "source": [
        "def hyper_params_test(activation, opt, batch_norm, regularization, dropout, lr_scheduler, data): \n",
        "    \n",
        "    model = MLP(input_size, hidden_size, num_classes, activation, batch_norm, dropout, regularization).to(device)\n",
        "    \n",
        "    # defining the loss function, optimiser, and learning rate scheduler\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    if opt == 'Adam':\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    elif opt == 'SGD':\n",
        "        optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "    elif opt == 'RMSprop':\n",
        "        optimizer = optim.RMSprop(model.parameters(), lr=0.001)\n",
        "\n",
        "    if lr_scheduler == 'ReduceLROnPlateau':\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=1, verbose=True)\n",
        "    elif lr_scheduler == 'StepLR':\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "    # training the model with initialiseed empty lists to store the accuracy, losses and itter\n",
        "    accuracy = []\n",
        "    losses = []\n",
        "    iter = []\n",
        "    startTime = time.time()\n",
        "    for epoch in range(epochs):\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in data:\n",
        "            # Move the images and labels to the device\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "             # forward pass to obtain predicted output\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            if regularization == 'l1':\n",
        "                l1_regularization = torch.tensor(0, dtype=torch.float32)\n",
        "                for param in model.parameters():\n",
        "                    l1_regularization += torch.norm(param, 1)\n",
        "                loss += 0.001 * l1_regularization\n",
        "            elif regularization == 'l2':\n",
        "                l2_regularization = torch.tensor(0, dtype=torch.float32)\n",
        "                for param in model.parameters():\n",
        "                    l2_regularization += torch.norm(param, 2)\n",
        "                loss += 0.01 * l2_regularization\n",
        "            else:\n",
        "                pass \n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # computing training accuracy\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        scheduler.step(loss)\n",
        "\n",
        "        # computing training accuracy for the epoch\n",
        "        Accuracy = correct / total\n",
        "        trainingAccuracy = 100 * Accuracy\n",
        "\n",
        "        # printing the loss and accuracy after each epoch\n",
        "#         print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}, Training Accuracy: {trainingAccuracy:.2f}%\")\n",
        "\n",
        "        accuracy.append(Accuracy)  # updating the list for storing accuracy values after \n",
        "        losses.append(loss.item())  # updating the list for storing loss values\n",
        "        iter.append(epoch+1)\n",
        "    \n",
        "    result = list(zip(iter, accuracy, losses))  # update the variable names in the zip function\n",
        "\n",
        "    endTime = time.time()\n",
        "    trainingTime = endTime - startTime\n",
        "#     print(f\"Training time: {trainingTime}\")\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lnqRnkwwEyc",
        "outputId": "b44f84cb-9e95-49ea-c18f-42eddd272b66"
      },
      "outputs": [],
      "source": [
        "train_result = hyper_params_test('relu', 'Adam', True, 'l1', 0.2, 'ReduceLROnPlateau', train_loader)\n",
        "\n",
        "train_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJXb_JPiwTY7",
        "outputId": "325e0b55-9fd2-49a9-d733-a544e9e9a5ed"
      },
      "outputs": [],
      "source": [
        "val_result = hyper_params_test('relu', 'Adam', True, 'l1', 0.2, 'ReduceLROnPlateau', val_loader)\n",
        "\n",
        "val_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sC5iFiIkALVv"
      },
      "outputs": [],
      "source": [
        "def random_search_hyperparams(num_trials, data):\n",
        "    startTime = time.time()\n",
        "    # defining the range of values for each hyperparameter used in the MLP traning\n",
        "    hyperparams = {\n",
        "        'activation': ['relu', 'sigmoid', 'tanh'], \n",
        "        'opt': ['SGD', 'Adam', 'RMSprop'],\n",
        "        'batch_norm': [True, False],\n",
        "        'regularization': ['l1', 'l2', 'none'],\n",
        "        'dropout': [0.0, 0.2],\n",
        "        'scheduler': ['ReduceLROnPlateau', 'StepLR']\n",
        "    }\n",
        "\n",
        "    best_accuracy = 0.0\n",
        "    best_hyperparams = None\n",
        "\n",
        "    for i in range(num_trials):\n",
        "        # Sample a set of hyperparameters at random\n",
        "        activation = random.choice(hyperparams['activation'])\n",
        "        opt = random.choice(hyperparams['opt'])\n",
        "        batch_norm = random.choice(hyperparams['batch_norm'])\n",
        "        regularization = random.choice(hyperparams['regularization'])\n",
        "        dropout = random.uniform(*hyperparams['dropout'])\n",
        "        scheduler = random.choice(hyperparams['scheduler'])\n",
        "\n",
        "        # Test the current set of hyperparameters\n",
        "        #result = hyper_params_test(activation, opt, batch_norm, regularization, dropout, scheduler, data)\n",
        "\n",
        "        # Compute the accuracy of the model for the current set of hyperparameters\n",
        "        accuracy = result[-1][1]\n",
        "\n",
        "        # Check if the current set of hyperparameters has produced the best accuracy so far\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "            best_hyperparams = {\n",
        "                'activation': activation,\n",
        "                'opt': opt,\n",
        "                'batch_norm': batch_norm,\n",
        "                'regularization': regularization,\n",
        "                'dropout': dropout,\n",
        "                'scheduler': scheduler\n",
        "            }\n",
        "    endTime = time.time()\n",
        "    trainingTime = endTime - startTime\n",
        "    return best_hyperparams, best_accuracy, trainingTime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqfocHVPvu0h"
      },
      "outputs": [],
      "source": [
        "best_hyperparams, best_accuracy, trainingTime = random_search_hyperparams(3, train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJFRSMRsA1UI",
        "outputId": "6c4ecba9-6690-4485-f09b-6fbc54b92d7a"
      },
      "outputs": [],
      "source": [
        "trainingTime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqN7DL_aA-j6",
        "outputId": "72d56fad-8cef-4467-adc5-10b3cf32862b"
      },
      "outputs": [],
      "source": [
        "best_hyperparams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFmxZlMbBbV5",
        "outputId": "b728fe42-58de-4df2-e30f-d6360ab63363"
      },
      "outputs": [],
      "source": [
        "best_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yneMQQr7vlkc",
        "outputId": "e8a9973e-6bdc-4476-c16e-70d21f4f422a"
      },
      "outputs": [],
      "source": [
        "best_hyperparams = best_hyperparams\n",
        "best_result = hyper_params_test(best_hyperparams['activation'], best_hyperparams['opt'], best_hyperparams['batch_norm'], best_hyperparams['regularization'], best_hyperparams['dropout'], best_hyperparams['scheduler'], train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "BAIaZCJwBw-o",
        "outputId": "a5200ab2-880c-462a-c9e2-9fda1749c6d5"
      },
      "outputs": [],
      "source": [
        "# getting the first two items in each tuple and store in separate lists\n",
        "train_iter = [x[0] for x in best_result]\n",
        "train_accuracy = [x[1] for x in best_result]\n",
        "val_iter = [x[0] for x in val_result]\n",
        "val_accuracy = [x[1] for x in val_result]\n",
        "\n",
        "# plotting a graph for both training and validation \n",
        "plt.plot(train_iter, train_accuracy, '-o', label='Training')\n",
        "plt.plot(val_iter, val_accuracy, '-o', label ='Validation')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy graph')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "qEQe2LC0D2EU",
        "outputId": "324bf185-cf1f-4b93-8e08-6085129514db"
      },
      "outputs": [],
      "source": [
        "# Get the first two items in each tuple and store in separate lists\n",
        "train_iter = [x[0] for x in best_result]\n",
        "train_accuracy = [x[2] for x in best_result]\n",
        "\n",
        "# Plot elbow graph for both training and validation loss\n",
        "plt.plot(train_iter, train_accuracy, '-o', label='Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss function Plot')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vupKF_w3sHry"
      },
      "outputs": [],
      "source": [
        "# Save the model state dictionary\n",
        "torch.save(model.state_dict(), 'mlp_model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_mHthF11Iiu",
        "outputId": "764a3e05-9710-4510-ee67-3aa7b7acefe4"
      },
      "outputs": [],
      "source": [
        "trainingTime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKhKoeV4DfWv"
      },
      "source": [
        "Convelutional Neural Network (CNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ghJyZNhDecd"
      },
      "outputs": [],
      "source": [
        "#defining the CNN model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, input_size, num_classes, activation, batch_norm, dropout, regularization):\n",
        "        super(CNN, self).__init__()\n",
        "        # convolutional layer 1 and max pool layer 1\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1, padding=2)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        # convolutional layer 2\n",
        "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1, padding=0)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2) \n",
        "        # fully connected layers and max pool layer 1\n",
        "        self.fc1 = nn.Linear(in_features=16 * 5 * 5, out_features=120)\n",
        "        self.fc2 = nn.Linear(in_features=120, out_features=84)\n",
        "        self.fc3 = nn.Linear(in_features=84, out_features=47)\n",
        "        self.activation = activation\n",
        "        self.batch_norm = batch_norm\n",
        "        self.dropout = dropout\n",
        "        self.regularization = regularization\n",
        "        if self.batch_norm:\n",
        "            self.bn1 = nn.BatchNorm2d(6)\n",
        "            self.bn2 = nn.BatchNorm2d(16)\n",
        "        if self.regularization:\n",
        "            self.l1 = nn.L1Loss()\n",
        "            self.l2 = nn.MSELoss()\n",
        "\n",
        "    def forward(self, x):\n",
        "       # convolutional layer 1\n",
        "        out = self.conv1(x)\n",
        "        if self.batch_norm:\n",
        "            out = self.bn1(out)\n",
        "        if self.activation == 'relu':\n",
        "            out = F.relu(out)\n",
        "        elif self.activation == 'sigmoid':\n",
        "            out = F.sigmoid(out)\n",
        "        elif self.activation == 'tanh':\n",
        "            out = F.tanh(out)\n",
        "        # max pool layer 1\n",
        "        out = self.pool(out)\n",
        "        # convolutional layer 2\n",
        "        out = self.conv2(out)\n",
        "        if self.batch_norm:\n",
        "            out = self.bn2(out)\n",
        "        if self.activation == 'relu':\n",
        "            out = F.relu(out)\n",
        "        elif self.activation == 'sigmoid':\n",
        "            out = F.sigmoid(out)\n",
        "        elif self.activation == 'tanh':\n",
        "            out = F.tanh(out)\n",
        "        # max pool layer 2\n",
        "        out = self.pool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc1(out)\n",
        "        if self.activation == 'relu':\n",
        "            out = F.relu(out)\n",
        "        elif self.activation == 'sigmoid':\n",
        "            out = F.sigmoid(out)\n",
        "        elif self.activation == 'tanh':\n",
        "            out = F.tanh(out)\n",
        "        if self.dropout:\n",
        "            out = F.dropout(out, p=self.dropout)\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnxu9r-OEgO9",
        "outputId": "f3fbccf0-91a7-4d93-e7bf-d492f8891fbe"
      },
      "outputs": [],
      "source": [
        "# inililising the CNN model for the traning function\n",
        "model = CNN(input_size, num_classes, activation, batch_norm, dropout, regularization).to(device)\n",
        "\n",
        "# defining the optimiser, learning rate (lr) and loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimiser = optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimiser, mode='min', factor=0.1, patience=1, verbose=True)\n",
        "\n",
        "accuracy, losses, epochs = [], [], epochs\n",
        "\n",
        "# Training the model\n",
        "start_time = time.time()\n",
        "for epoch in range(epochs):\n",
        "    correct, total = 0, 0\n",
        "    for images, labels in train_loader:\n",
        "        # Move the images and labels to the device\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # regularisation\n",
        "        if regularization in ('l1', 'l2'):\n",
        "            reg_loss = sum(torch.norm(param, p=int(regularization[1])) for param in model.parameters())\n",
        "            loss += regularization * reg_loss\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimiser.zero_grad()\n",
        "        loss.backward()\n",
        "        optimiser.step()\n",
        "\n",
        "        # Compute training accuracy\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        # the total number of labels\n",
        "        total += labels.size(0)\n",
        "        # the total correct predictions\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    scheduler.step(loss)\n",
        "\n",
        "    # Compute training accuracy for the epoch\n",
        "    accuracy.append(correct / total)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    # printing the loss and accuracy after each epoch \n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}, Training Accuracy: {accuracy[-1] * 100:.2f}%\")\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "print(f\"Training time: {training_time:.2f}s\")\n",
        "\n",
        "result = list(zip(range(epochs), accuracy, losses))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCh2vJrB2mSD"
      },
      "outputs": [],
      "source": [
        "def hyper_params_test(activation, opt, batch_norm, regularization, dropout, lr_scheduler, data): \n",
        "    \n",
        "    model = CNN(input_size, num_classes, activation, batch_norm, dropout, regularization).to(device)\n",
        "    \n",
        "    # Define the loss function, optimizer, and learning rate scheduler\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    if opt == 'Adam':\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    elif opt == 'SGD':\n",
        "        optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "    elif opt == 'RMSprop':\n",
        "        optimizer = optim.RMSprop(model.parameters(), lr=0.001)\n",
        "\n",
        "    if lr_scheduler == 'ReduceLROnPlateau':\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=1, verbose=True)\n",
        "    elif lr_scheduler == 'StepLR':\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "    # training the CNN model\n",
        "    # initialise empty lists to store the accuracies, losses, iter \n",
        "    accuracy = []\n",
        "    losses = []\n",
        "    iter = []\n",
        "    startTime = time.time()\n",
        "    # loop over the number of epochs specified\n",
        "    for epoch in range(epochs):\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in data:\n",
        "            # Move the images and labels to the device\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            if regularization == 'l1':\n",
        "                l1_regularization = torch.tensor(0, dtype=torch.float32)\n",
        "                for param in model.parameters():\n",
        "                    l1_regularization += torch.norm(param, 1)\n",
        "                loss += 0.001 * l1_regularization\n",
        "            elif regularization == 'l2':\n",
        "                l2_regularization = torch.tensor(0, dtype=torch.float32)\n",
        "                for param in model.parameters():\n",
        "                    l2_regularization += torch.norm(param, 2)\n",
        "                loss += 0.01 * l2_regularization\n",
        "            else:\n",
        "                pass  # Do nothing if regularization is None\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Compute training accuracy\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        scheduler.step(loss)\n",
        "\n",
        "        # Compute training accuracy for the epoch\n",
        "        Accuracy = correct / total\n",
        "        trainingAccuracy = 100 * Accuracy\n",
        "\n",
        "        # Print the loss and accuracy after each epoch\n",
        "#         print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}, Training Accuracy: {trainingAccuracy:.2f}%\")\n",
        "\n",
        "        accuracy.append(Accuracy)  # Update the list for storing accuracy values\n",
        "        losses.append(loss.item())  # Update the list for storing loss values\n",
        "        iter.append(epoch+1)\n",
        "    \n",
        "    result = list(zip(iter, accuracy, losses))  # Update variable name in the zip function\n",
        "\n",
        "    endTime = time.time()\n",
        "    trainingTime = endTime - startTime\n",
        "#     print(f\"Training time: {trainingTime}\")\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4MDExet2n8e",
        "outputId": "1f53195f-8f6c-446d-ee7b-bc249a62963d"
      },
      "outputs": [],
      "source": [
        "train_result = hyper_params_test('relu', 'Adam', True, 'l1', 0.2, 'ReduceLROnPlateau', train_loader)\n",
        "train_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzTVXO3r2oHO",
        "outputId": "bb22d82a-f69f-4727-e44e-af5e6745839b"
      },
      "outputs": [],
      "source": [
        "val_result = hyper_params_test('relu', 'Adam', True, 'l1', 0.2, 'ReduceLROnPlateau', val_loader)\n",
        "val_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPK0h2w5EgRu"
      },
      "outputs": [],
      "source": [
        "def random_search_hyperparams(num_trials, data):\n",
        "    startTime = time.time()\n",
        "    # defining the range of values for each of the hyperparameter\n",
        "    hyperparams = {\n",
        "        'activation': ['relu', 'sigmoid', 'tanh'], \n",
        "        'opt': ['SGD', 'Adam', 'RMSprop'],\n",
        "        'batch_norm': [True, False],\n",
        "        'regularization': ['l1', 'l2', 'none'],\n",
        "        'dropout': [0.0, 0.2],\n",
        "        'scheduler': ['ReduceLROnPlateau', 'StepLR']\n",
        "    }\n",
        "\n",
        "    best_accuracy = 0.0\n",
        "    best_hyperparams = None\n",
        "\n",
        "    for i in range(num_trials):\n",
        "        # sample a set of hyperparameters at random to start\n",
        "        activation = random.choice(hyperparams['activation'])\n",
        "        opt = random.choice(hyperparams['opt'])\n",
        "        batch_norm = random.choice(hyperparams['batch_norm'])\n",
        "        regularization = random.choice(hyperparams['regularization'])\n",
        "        dropout = random.uniform(*hyperparams['dropout'])\n",
        "        scheduler = random.choice(hyperparams['scheduler'])\n",
        "\n",
        "        # Test the current set of hyperparameters\n",
        "        result = hyper_params_test(activation, opt, batch_norm, regularization, dropout, scheduler, data)\n",
        "\n",
        "        # Compute the accuracy of the model for the current set of hyperparameters\n",
        "        accuracy = result[-1][1]\n",
        "\n",
        "        # Check if the current set of hyperparameters has produced the best accuracy so far\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "            best_hyperparams = {\n",
        "                'activation': activation,\n",
        "                'opt': opt,\n",
        "                'batch_norm': batch_norm,\n",
        "                'regularization': regularization,\n",
        "                'dropout': dropout,\n",
        "                'scheduler': scheduler\n",
        "            }\n",
        "    endTime = time.time()\n",
        "    trainingTime = endTime - startTime\n",
        "    return best_hyperparams, best_accuracy, trainingTime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9UMSR5J2hfT",
        "outputId": "17b1f0ae-9248-497e-aba2-b163b0ef16db"
      },
      "outputs": [],
      "source": [
        "best_hyperparams, best_accuracy, trainingTime = random_search_hyperparams(3, train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXoUzvFiEgUj",
        "outputId": "e4c53156-ff0b-4bd3-d7b1-9d7429e65d27"
      },
      "outputs": [],
      "source": [
        "trainingTime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHq4BFToEgXK",
        "outputId": "a735c41f-9238-4ff0-9770-2bf7b528efeb"
      },
      "outputs": [],
      "source": [
        "best_hyperparams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ASyxAgXKB-w",
        "outputId": "3b3e109a-2f09-4eb9-b941-957a1dbec241"
      },
      "outputs": [],
      "source": [
        "best_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "aMU4sMXVKHrS",
        "outputId": "04ae330a-020a-4b60-d639-32f9cd011a06"
      },
      "outputs": [],
      "source": [
        "#storing the first two items in each tuplein a separate lists\n",
        "train_iter = [x[0] for x in best_result]\n",
        "train_accuracy = [x[1] for x in best_result]\n",
        "val_iter = [y[0] for y in val_result]\n",
        "val_accuracy = [y[1] for y in val_result]\n",
        "\n",
        "# plotting a graph for both training and validation loss\n",
        "plt.plot(train_iter, train_accuracy, '-o', label='Accuracy')\n",
        "plt.plot(val_iter, val_accuracy, '-o', label ='Validation')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy graph')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "2n1jhF634wQr",
        "outputId": "dfb8f454-aa09-4302-8b76-33a21736fb60"
      },
      "outputs": [],
      "source": [
        "#storing the first two items in each tuplein a separate lists\n",
        "train_iter = [x[0] for x in best_result]\n",
        "train_accuracy = [x[2] for x in best_result]\n",
        "val_iter = [x[0] for x in val_result]\n",
        "val_accuracy = [x[1] for x in val_result]\n",
        "\n",
        "# Extract the accuracy and losses from the result\n",
        "iter, accuracy, losses = zip(*result)\n",
        "\n",
        "# plotting a graph for both training and validation loss\n",
        "plt.plot(train_iter, train_accuracy, '-o', label='Training')\n",
        "plt.plot(val_iter, val_accuracy, '-o', label ='Validation')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss function graph')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-Yq9KEz7GzJ"
      },
      "outputs": [],
      "source": [
        "# saving the CNN model state in a dictionary\n",
        "torch.save(model.state_dict(), 'cnn_model.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wD--b2T7G4c",
        "outputId": "9b0e03a2-0241-40a8-bb18-4ad89185faf4"
      },
      "outputs": [],
      "source": [
        "trainingTime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcv5VjwD7HDo"
      },
      "source": [
        "STEP 6:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-j1bGpUDSbS",
        "outputId": "59a2d59f-163c-4eb9-ef42-c4a4f7874a57"
      },
      "outputs": [],
      "source": [
        " # loading the saved MLP model\n",
        "mlp_model = MLP(input_size, hidden_size, num_classes, activation, batch_norm, dropout, regularization).to(device)\n",
        "mlp_model.load_state_dict(torch.load('mlp_model.pth'))\n",
        "mlp_model.eval()\n",
        "\n",
        "# running the testing data through the saved MLP model\n",
        "test_accuracy = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = mlp_model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        test_accuracy += (predicted == labels).sum().item()\n",
        "\n",
        "# computing the test data accuracy\n",
        "Accuracy = test_accuracy / total\n",
        "testAccuracy = 100 * Accuracy\n",
        "print(f\"Test Accuracy: {testAccuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-B7WKzVntYzO",
        "outputId": "96264574-f95f-43b2-e831-650b0c8bd0db"
      },
      "outputs": [],
      "source": [
        " # loading the saved CNN model\n",
        "cnn_model = CNN(input_size, num_classes, activation, batch_norm, dropout, regularization).to(device)\n",
        "cnn_model.load_state_dict(torch.load('cnn_model.pth'))\n",
        "cnn_model.eval()\n",
        "\n",
        "# running the testing data through the saved CNN model\n",
        "test_accuracy = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = cnn_model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        test_accuracy += (predicted == labels).sum().item()\n",
        "\n",
        "# computing the test data accuracy\n",
        "Accuracy = test_accuracy / total\n",
        "testAccuracy = 100 * Accuracy\n",
        "print(f\"Test Accuracy: {testAccuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzWwfp-K7-o1"
      },
      "source": [
        "Top 6 samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "px0QWk8V7_By",
        "outputId": "f1a71b7d-47a6-473a-e945-47a53dd9a8b6"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        # Move the images and labels to the device\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = cnn_model(images)      \n",
        "        _, predicted = torch.max(outputs.data, 1) # Get the predicted classes  \n",
        "\n",
        "        for i in range(6): # printing the top six predictions along with the true labels\n",
        "            print(f\"Predicted: {predicted[i]}, True Label: {labels[i]}\")\n",
        "\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-f20HX3u7_Es",
        "outputId": "836b75cf-0ab9-4026-a19f-c6fd0de8367a"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        # moving the images and labels to the device\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = cnn_model(images)       \n",
        "        _, predicted = torch.max(outputs.data, 1) # Get the predicted classes\n",
        "       \n",
        "        for i in range(5):  # printing the top 5 predictions along with the true labels\n",
        "            print(f\"Predicted: {predicted[i]}, True Label: {labels[i]}\")\n",
        "            plt.imshow(images[i].cpu().numpy().squeeze(), cmap='gray')\n",
        "            plt.show()\n",
        "\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwJQEHGi8Xqg"
      },
      "source": [
        "Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyQqOxhgC6rl"
      },
      "outputs": [],
      "source": [
        "# get true labels\n",
        "true_labels = []\n",
        "for _, label in test_loader:\n",
        "    true_labels += label.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Z0VqwF6DCvi"
      },
      "outputs": [],
      "source": [
        "# make predictions using MLP model\n",
        "mlp_predictions = []\n",
        "mlp_model.eval()\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        outputs = mlp_model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        mlp_predictions += predicted.tolist()\n",
        "# create confusion matrices\n",
        "mlp_cm = confusion_matrix(true_labels, mlp_predictions)\n",
        "\n",
        "# make predictions using CNN model\n",
        "cnn_predictions = []\n",
        "cnn_model.eval()\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        outputs = cnn_model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        cnn_predictions += predicted.tolist()\n",
        "        \n",
        "# create confusion matrices\n",
        "cnn_cm = confusion_matrix(true_labels, cnn_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "U8ltspX38dui",
        "outputId": "331ae081-5d1a-481a-828d-f295eb2d3c22"
      },
      "outputs": [],
      "source": [
        "# plotting the confusion matrices for both models\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(30, 30))\n",
        "\n",
        "# MLP confusion matrix\n",
        "sns.heatmap(mlp_cm, annot=True, fmt='g', cmap='OrRd', ax=ax1)\n",
        "ax1.set_xlabel('Predicted', weight='bold')\n",
        "ax1.set_ylabel('True', weight='bold')\n",
        "ax1.set_title('MLP Confusion Matrix')\n",
        "\n",
        "# CNN confusion matrix\n",
        "sns.heatmap(cnn_cm, annot=True, fmt='g', cmap='OrRd', ax=ax2)\n",
        "ax2.set_xlabel('Predicted', weight='bold')\n",
        "ax2.set_ylabel('True', weight='bold')\n",
        "ax2.set_title('CNN Confusion Matrix')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TMdwc9T8scr",
        "outputId": "2129490d-19c1-4f3c-c847-ec18c26e38d9"
      },
      "outputs": [],
      "source": [
        "# setting up the model to evaluation mode for MLP to print the precision, recall, F1 and support for the models \n",
        "mlp_model.eval()\n",
        "\n",
        "# defining the lists to store the true and predicted labels\n",
        "true_labels = []\n",
        "pred_labels = []\n",
        "\n",
        "# looping through the test data and generate predictions\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        # move the images and labels to the device\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        # append the true and predicted labels to the lists\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "        pred_labels.extend(predicted.cpu().numpy())\n",
        "\n",
        "# printing the classification report with the  precision, recall, F1 and support \n",
        "print(classification_report(true_labels, pred_labels))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHiXoIiS8skO",
        "outputId": "a8faf501-fd56-4848-bd99-5423b92ee030"
      },
      "outputs": [],
      "source": [
        "# settin up the model to evaluation mode for CNN to print the precision, recall, F1 and support for the models \n",
        "cnn_model.eval()\n",
        "\n",
        "# defining the lists to store the true and predicted labels\n",
        "true_labels = []\n",
        "pred_labels = []\n",
        "\n",
        "# looping through the test data and generate predictions\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        # move the images and labels to the device\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        # append the true and predicted labels to the lists\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "        pred_labels.extend(predicted.cpu().numpy())\n",
        "\n",
        "# print the classification report with the  precision, recall, F1 and support \n",
        "print(classification_report(true_labels, pred_labels))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
